{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "c9wnXBnt_HBe",
        "outputId": "897ae31e-3757-4e34-a771-39cf2f8eff5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3256fda4-bbe7-46ed-907d-8052dd40dc61\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3256fda4-bbe7-46ed-907d-8052dd40dc61\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving training_df.tsv to training_df.tsv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, PrecisionRecallDisplay, accuracy_score, matthews_corrcoef, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASGZSuX0Saxc"
      },
      "source": [
        "# Functions for Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sequence_composition(sequence:str) -> np.ndarray[np.float64]:\n",
        "  sequence = sequence[:40]\n",
        "  alphabet = \"AQLSREKTNGMWDHFYCIPV\"\n",
        "  composition_matrix = np.zeros((20,40))\n",
        "  composition_vector = np.zeros((20))\n",
        "  for i in range(len(alphabet)):\n",
        "    for j in range(len(sequence)):\n",
        "      if alphabet[i] == sequence[j]:\n",
        "        composition_matrix[i][j] += 1\n",
        "        composition_vector[i] += composition_matrix[i][j]\n",
        "  composition_vector = composition_vector/40\n",
        "  return composition_vector\n",
        "\n",
        "def hp_ai(sequence:str, window_size:int = 6 ) -> np.ndarray[np.float64]:\n",
        "  kd_scale = {\n",
        "    'R': -4.5,  # Arg\n",
        "    'K': -3.9,  # Lys\n",
        "    'N': -3.5,  # Asn\n",
        "    'D': -3.5,  # Asp\n",
        "    'Q': -3.5,  # Gln\n",
        "    'E': -3.5,  # Glu\n",
        "    'H': -3.2,  # His\n",
        "    'P': -1.6,  # Pro\n",
        "    'Y': -1.3,  # Tyr\n",
        "    'W': -0.9,  # Trp\n",
        "    'S': -0.8,  # Ser\n",
        "    'T': -0.7,  # Thr\n",
        "    'G': -0.4,  # Gly\n",
        "    'A':  1.8,  # Ala\n",
        "    'M':  1.9,  # Met\n",
        "    'C':  2.5,  # Cys\n",
        "    'F':  2.8,  # Phe\n",
        "    'L':  3.8,  # Leu\n",
        "    'V':  4.2,  # Val\n",
        "    'I':  4.5   # Ile\n",
        "    }\n",
        "\n",
        "  a = 2.9 # relative volume of valine\n",
        "  b = 3.9 # relative volume leucine/isoleucine\n",
        "\n",
        "  sequence = sequence[:40]\n",
        "  # padding to have as many scores as residues  (each window refers to the central res)\n",
        "  d = int(window_size/2)\n",
        "  sequence = \"X\"*d + sequence + \"X\"*d #padding\n",
        "  hydrophobicities = np.array([])\n",
        "  AIs = np.array([])  #aliphatic indexes\n",
        "  for i in range(len(sequence)-(window_size)+1):\n",
        "      counts_A = 0\n",
        "      counts_V = 0\n",
        "      counts_I = 0\n",
        "      counts_L = 0\n",
        "      hydrophobicity_score = 0\n",
        "      window = sequence[i:i+window_size]\n",
        "      for j in range(window_size):\n",
        "        if window[j] != \"X\":\n",
        "          hydrophobicity_score = (hydrophobicity_score + kd_scale[window[j]])\n",
        "          if window[j] == 'A':\n",
        "              counts_A += 1\n",
        "          elif window[j] == 'V':\n",
        "              counts_V += 1\n",
        "          elif window[j] == 'I':\n",
        "              counts_I += 1\n",
        "          elif window[j] == 'L':\n",
        "                counts_L += 1\n",
        "      X_A = (counts_A / window_size)\n",
        "      X_V = (counts_V / window_size)\n",
        "      X_I = (counts_I / window_size)\n",
        "      X_L = (counts_L / window_size)\n",
        "      AI = X_A + a * X_V + b * (X_I + X_L) #Aliphatic Index Formula\n",
        "      hydrophobicities = np.append(hydrophobicities, hydrophobicity_score / window_size)\n",
        "      AIs = np.append(AIs, AI)\n",
        "  H_AI = np.array([hydrophobicities.mean(), hydrophobicities.max(), np.argmax(hydrophobicities), AIs.mean(), AIs.max(), np.argmax(AIs)])\n",
        "  return H_AI  #hydrophobicity and aliphatic index alltogether\n",
        "\n",
        "def SSE(sequence,window_size):\n",
        "  alpha_helix_scale = {\n",
        "      \"G\": 0.570,  # Gly\n",
        "      \"P\": 0.570,  # Pro\n",
        "      \"Y\": 0.690,  # Tyr\n",
        "      \"C\": 0.700,  # Cys\n",
        "      \"S\": 0.770,  # Ser\n",
        "      \"T\": 0.830,  # Thr\n",
        "      \"N\": 0.670,  # Asn\n",
        "      \"R\": 0.980,  # Arg\n",
        "      \"H\": 1.000,  # His\n",
        "      \"D\": 1.010,  # Asp\n",
        "      \"I\": 1.080,  # Ile\n",
        "      \"W\": 1.080,  # Trp\n",
        "      \"Q\": 1.110,  # Gln\n",
        "      \"F\": 1.130,  # Phe\n",
        "      \"K\": 1.160,  # Lys\n",
        "      \"V\": 1.060,  # Val\n",
        "      \"L\": 1.210,  # Leu\n",
        "      \"A\": 1.420,  # Ala\n",
        "      \"M\": 1.450,  # Met\n",
        "      \"E\": 1.510   # Glu\n",
        "  }\n",
        "  beta_sheet_scale = {\n",
        "      \"E\": 0.370,  # Glu\n",
        "      \"D\": 0.540,  # Asp\n",
        "      \"P\": 0.550,  # Pro\n",
        "      \"G\": 0.750,  # Gly\n",
        "      \"S\": 0.750,  # Ser\n",
        "      \"K\": 0.740,  # Lys\n",
        "      \"H\": 0.870,  # His\n",
        "      \"N\": 0.890,  # Asn\n",
        "      \"R\": 0.930,  # Arg\n",
        "      \"A\": 0.830,  # Ala\n",
        "      \"M\": 1.050,  # Met\n",
        "      \"Q\": 1.100,  # Gln\n",
        "      \"C\": 1.190,  # Cys\n",
        "      \"T\": 1.190,  # Thr\n",
        "      \"L\": 1.300,  # Leu\n",
        "      \"F\": 1.380,  # Phe\n",
        "      \"W\": 1.370,  # Trp\n",
        "      \"Y\": 1.470,  # Tyr\n",
        "      \"I\": 1.600,  # Ile\n",
        "      \"V\": 1.700   # Val\n",
        "  }\n",
        "  sequence = sequence[:40]\n",
        "  # padding to have as many scores as residues (each window refers to the central res)\n",
        "  d = int(window_size/2)\n",
        "  sequence = \"X\"*d + sequence + \"X\"*d #padding\n",
        "  alpha_helix = np.array([])\n",
        "  beta_sheet = np.array([])\n",
        "  for i in range(len(sequence)-(window_size)+1):\n",
        "      alpha_score = 0\n",
        "      beta_score = 0\n",
        "      window = sequence[i:i+window_size]\n",
        "      for j in range(window_size):\n",
        "        w = 1 - abs(j - (window_size - 1)/2) / ((window_size - 1)/2)\n",
        "        if window[j] != \"X\":\n",
        "          alpha_score = alpha_score + w * alpha_helix_scale[window[j]]\n",
        "          beta_score = beta_score + w * beta_sheet_scale[window[j]]\n",
        "      alpha_helix = np.append(alpha_helix, alpha_score / window_size)\n",
        "      beta_sheet = np.append(beta_sheet, beta_score / window_size)\n",
        "  alpha_feature = np.array([alpha_helix.mean(), alpha_helix.max(), np.argmax(alpha_helix)])\n",
        "  beta_feature = np.array([beta_sheet.mean(), beta_sheet.max(), np.argmax(beta_sheet)])\n",
        "  return alpha_helix, alpha_feature, beta_sheet, beta_feature\n",
        "\n",
        "def charge_seq(sequence, window_size):\n",
        "  res_charges = {\n",
        "        'K': 1,   # Lys\n",
        "        'R': 1,   # Arg\n",
        "        'H': 0.5, # His (partial positive)\n",
        "        'D': -1,  # Asp\n",
        "        'E': -1   # Glu\n",
        "    }\n",
        "\n",
        "  sequence = sequence[:20]\n",
        "  # padding to have as many scores as residues  (each window refers to the central res)\n",
        "  d = int(window_size/2)\n",
        "  sequence = \"X\"*d + sequence + \"X\"*d #padding\n",
        "\n",
        "  norm_charges = np.array([])\n",
        "\n",
        "  for i in range(len(sequence)-(window_size)+1):\n",
        "      charge = 0\n",
        "      window = sequence[i:i+window_size]\n",
        "      for j in range(window_size):\n",
        "        if window[j] != \"X\" and window[j] in res_charges:\n",
        "          charge = (charge + res_charges[window[j]])\n",
        "      norm_charges = np.append(norm_charges, charge / window_size)\n",
        "  charge_feature = np.array([norm_charges.max(), np.argmax(norm_charges), norm_charges.min(), np.argmin(norm_charges)])\n",
        "  return norm_charges, charge_feature\n",
        "\n",
        "def extract_features(seqs):\n",
        "  features = np.empty([len(seqs), 36])\n",
        "\n",
        "  for row, seq in enumerate(seqs):\n",
        "    feature = np.array(())\n",
        "    aa_comp = sequence_composition(seq)\n",
        "    hyd_ali_index = hp_ai(seq)\n",
        "    _, alpha, _, beta = SSE(seq, window_size=9)\n",
        "    _, charges = charge_seq(seq, window_size=3)\n",
        "\n",
        "    feature = np.concatenate((aa_comp, hyd_ali_index, alpha, beta, charges), axis = None)\n",
        "\n",
        "    features[row] = feature\n",
        "# Return individual feature vectors\n",
        "  return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross validation cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_val(cycle_number:int):\n",
        "  validation = []\n",
        "  testing = []\n",
        "  training = []\n",
        "  for j in range(cycle_number):\n",
        "    validation.append(j+1)\n",
        "    testing.append(((j+1)%cycle_number)+1)\n",
        "    training_j = []\n",
        "    for i in range(cycle_number):\n",
        "      if i != j and i != (j+1)%cycle_number:\n",
        "        training_j.append(i+1)\n",
        "    training.append(training_j)\n",
        "    j +=1\n",
        "  return training, validation, testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature extraction for the whole ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = ([f\"Composition({residue})\" for residue in \"AQLSREKTNGMWDHFYCIPV\"] + # 20 aa composition\n",
        " [\"mean_hydrophobicity\", \"max_hydrophobicity\", \"most_hydrophobic_residue\",\n",
        "  \"mean_aliphacity\", \"max_aliphacity\", \"most_aliphatic_residue\", # 6 hydrophobicity and aliphacity\n",
        "  \"mean_alpha_helix\", \"max_alpha_helix\", \"most_likely_alpha_helix\",\n",
        "  \"mean_beta_sheet\", \"max_beta_sheet\", \"most_likely_beta_sheet\", # 6 accounting for SSE\n",
        "  \"max_charge\", \"most_charged\", \"min_charge\", \"less_charged\" # 4 accounting for charge\n",
        "  ])\n",
        "\n",
        "# CREATING PANDAS SERIES FOR SEQUENCES AND CLASSES\n",
        "df = pd.read_csv(\"training_df.tsv\", sep = \"\\t\")\n",
        "training,validation,testing = [1,2,3],4,5\n",
        "training_sequences = pd.Series([], dtype = object)\n",
        "training_classes = pd.Series([], dtype = object)\n",
        "validation_sequences = df[df['Label'] == validation]['Frag_90']\n",
        "validation_classes = df[df['Label'] == validation]['Class']\n",
        "testing_sequences = df[df['Label'] == testing]['Frag_90']\n",
        "testing_classes = df[df['Label'] == testing]['Class']\n",
        "for el in training:\n",
        "        df_training = df[df['Label'] == el]\n",
        "        training_sequences = pd.concat([training_sequences, df_training['Frag_90']])\n",
        "        training_classes = pd.concat([training_classes, df_training['Class']])\n",
        "\n",
        "#print(type(training_sequences), type(training_classes),\n",
        "#      type(validation_sequences), type(validation_classes),\n",
        "#      type(testing_sequences), type(testing_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FEATURE EXTRACTION\n",
        "training_features = extract_features(training_sequences)\n",
        "validation_features = extract_features(validation_sequences)\n",
        "testing_features = extract_features(testing_sequences)\n",
        "print(training_features.shape)\n",
        "print(\"\")\n",
        "print(validation_features.shape)\n",
        "print(\"\")\n",
        "print(testing_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FEATURE SCALING\n",
        "def feature_scaler(features):\n",
        "  scaler = StandardScaler()\n",
        "  scaled_feature = scaler.fit_transform(features)\n",
        "  return scaled_feature\n",
        "training_features, validation_features, testing_features = feature_scaler(training_features), feature_scaler(validation_features), feature_scaler(testing_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline SVM (all features, manual grid over C and gamma on validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline SVM (all features) â€” manual grid over C and gamma on validation\n",
        "\n",
        "def svm_pipeline(C, gamma, kernel):\n",
        "    return Pipeline([\n",
        "        (\"svm\", SVC(kernel=kernel, C=C, gamma=gamma, random_state=42))\n",
        "    ])\n",
        "\n",
        "# grid parameters\n",
        "kernels = [\"linear\", \"rbf\", \"poly\"]\n",
        "C_grid = [0.1, 1, 10, 100]\n",
        "gamma_grid = [\"scale\", 0.01, 0.1, 1.0]\n",
        "\n",
        "best_score_base = -np.inf\n",
        "best_params_base = None\n",
        "\n",
        "for kernel in kernels:\n",
        "  for C in C_grid:\n",
        "      for gamma in gamma_grid:\n",
        "          pipe = svm_pipeline(C, gamma, kernel)\n",
        "          pipe = pipe.fit(training_features, training_classes)                 # fit on TRAIN\n",
        "          pred_val = pipe.predict(validation_features)\n",
        "          val_f1 = f1_score(validation_classes, pred_val)         # evaluate on VALIDATION\n",
        "          if val_f1 > best_score_base:\n",
        "              best_score_base = val_f1\n",
        "              best_params_base = {\"C\": C, \"gamma\": gamma, \"kernel\": kernel}\n",
        "\n",
        "print(\"Baseline SVM (all features) â€” best validation F1 score: \"\n",
        "      f\"{best_score_base:.3f} with params {best_params_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random forest gini importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INITIALIZING RANDOM FOREST CLASSIFIER\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight={0: 9, 1: 1} # ADDRESSING CLASS UNBALANCE\n",
        ")\n",
        "rf.fit(training_features, training_classes)  # fit only on TRAIN\n",
        "gini_imp = pd.Series(rf.feature_importances_, index = feature_names).sort_values(ascending=False)\n",
        "gini_df = gini_imp.reset_index()\n",
        "gini_df.columns = [\"feature\", \"importance\"]\n",
        "print(\"Top 15 features by Gini importance:\")\n",
        "print(gini_df.head(15))\n",
        "\n",
        "# Plot top 20\n",
        "plt.figure()\n",
        "plt.barh(gini_df[\"feature\"].head(20)[::-1], gini_df[\"importance\"].head(20)[::-1])\n",
        "plt.xlabel(\"Gini importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"RandomForest Gini Importances (Top 20)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top k features selection based on gini importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f1_on_subset(C, gamma, kernel, subset_features):\n",
        "    # subset by feature names\n",
        "    feature_names_np = np.array(feature_names) # Convert to numpy array\n",
        "    idx = [np.where(feature_names_np == f)[0][0] for f in subset_features]\n",
        "    Xtr = training_features[:, idx]\n",
        "    Xva = validation_features[:, idx]\n",
        "    pipe = svm_pipeline(C, gamma, kernel)\n",
        "    pipe = pipe.fit(Xtr, training_classes)     # train on TRAIN only\n",
        "    pred = pipe.predict(Xva)     # predict on VAL only\n",
        "    return f1_score(validation_classes, pred)  # f1 on VALIDATION\n",
        "\n",
        "# We'll sweep k and, for each k, re-evaluate the best baseline SVM params on the reduced feature set\n",
        "ks = list(range(2, min(30, training_features.shape[1]+1)))  # keep it small for speed/clarity\n",
        "print(len(ks))\n",
        "curve = []\n",
        "\n",
        "for k in ks:\n",
        "    subset = gini_df[\"feature\"].head(k).tolist()\n",
        "    f1_k = f1_on_subset(best_params_base[\"C\"], best_params_base[\"gamma\"], best_params_base[\"kernel\"], subset)\n",
        "    curve.append(f1_k)\n",
        "\n",
        "best_k_idx = int(np.argmax(curve))\n",
        "best_k = ks[best_k_idx]\n",
        "print(f\"Best k on validation (using baseline best params): k={best_k}, val_f1={curve[best_k_idx]:.3f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(ks, curve, marker=\"o\")\n",
        "plt.xlabel(\"k (top features by RF Gini)\")\n",
        "plt.ylabel(\"Validation F1 score (SVM)\")\n",
        "plt.title(\"F1 score vs. Number of Selected Features (Validation set)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retuning the SVM on the selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the best k from the validation curve\n",
        "feature_names_np = np.array(feature_names)\n",
        "best_subset = gini_df[\"feature\"].head(best_k).tolist()\n",
        "idx = [np.where(feature_names_np == f)[0][0] for f in best_subset]\n",
        "\n",
        "Xtr_sel = training_features[:, idx]\n",
        "Xva_sel = validation_features[:, idx]\n",
        "Xte_sel = testing_features[:, idx]\n",
        "\n",
        "# Manual grid search again but now restricted to the selected features\n",
        "best_score_sel = -np.inf\n",
        "best_params_sel = None\n",
        "\n",
        "for kernel in kernels:\n",
        "  for C in C_grid:\n",
        "      for gamma in gamma_grid:\n",
        "          pipe = svm_pipeline(C, gamma, kernel)\n",
        "          pipe = pipe.fit(Xtr_sel, training_classes)      # train on TRAIN\n",
        "          pred = pipe.predict(Xva_sel)\n",
        "          val_f1 = f1_score(validation_classes, pred)  # validate on VAL\n",
        "          if val_f1 > best_score_sel:\n",
        "              best_score_sel = val_f1\n",
        "              best_params_sel = {\"C\": C, \"gamma\": gamma, \"kernel\": kernel}\n",
        "\n",
        "# Train final model on TRAIN+VAL with best params (optional) or just TRAIN; here we keep TRAIN only as per your outline\n",
        "final_pipe = svm_pipeline(best_params_sel[\"C\"], best_params_sel[\"gamma\"], best_params_sel[\"kernel\"])\n",
        "final_pipe = final_pipe.fit(Xtr_sel, training_classes)\n",
        "pred_test = final_pipe.predict(Xte_sel)\n",
        "test_f1 = f1_score(testing_classes, pred_test)\n",
        "\n",
        "print(\"Selected features (best k):\", best_subset)\n",
        "print(\"Best validation F1 score on selected features:\", f\"{best_score_sel:.3f}\", \"with\", best_params_sel)\n",
        "print(\"Test F1 score (selected features, tuned on val):\", f\"{test_f1:.3f}\")\n",
        "\n",
        "# For comparison: test accuracy with all features using baseline best params\n",
        "baseline_pipe = svm_pipeline(best_params_base[\"C\"], best_params_base[\"gamma\"], best_params_base[\"kernel\"])\n",
        "baseline_pipe.fit(training_features, training_classes)\n",
        "baseline_predtest = baseline_pipe.predict(testing_features)\n",
        "test_f1_all = matthews_corrcoef(testing_classes, baseline_predtest)\n",
        "print(\"Test F1 score (all features, baseline tuned on val):\", f\"{test_f1_all:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(Xtr_sel[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_MOWMxuGwWr",
        "outputId": "7463a4fb-3d55-4ef0-fc08-14becde5e444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8021, 5)\n"
          ]
        }
      ],
      "source": [
        "# --- DATASET IMPORT ---\n",
        "df = pd.read_csv(\"training_df.tsv\", sep = \"\\t\")\n",
        "\n",
        "# --- CROSS-VALIDATION SETUP ---\n",
        "cycles = 5\n",
        "training, validation, testing = cross_val(cycles)\n",
        "\n",
        "# --- INITIALIZE METRICS ---\n",
        "confusions = [[] for _ in range(cycles)]\n",
        "accuracies = np.array([])\n",
        "precisions = np.array([])\n",
        "recalls = np.array([])\n",
        "f1s = np.array([])\n",
        "mccs = np.array([])\n",
        "ths = np.array([])\n",
        "\n",
        "# --- CROSS-VALIDATION LOOP ---\n",
        "      # During each cross-validation run,\n",
        "      # each value in the grid is obtained with SVM models trained using the corresponding combination of C and ð² and tested on the validation set\n",
        "      # The best combination is selected as the one providingthe highest MCC on validation set\n",
        "\n",
        "    # -------------------- TRAINING --------------------\n",
        "      # FEATURE EXTRACTION\n",
        "      # DEFINE MODEL WITH EVERY COMBINATION POSSIBLE OF THE HYPERPARAMETERS\n",
        "    # -------------------- VALIDATION --------------------\n",
        "      # EVALUATE BEST PARAMETERS AS THE MODEL WITH THE HIGHES MCC\n",
        "      # KEEP THE MODEL WITH THE HIGHES MCC FOR TESTING\n",
        "    # -------------------- TESTING --------------------\n",
        "      # FEATURE EXTRACTION\n",
        "      # PREDICTION WITH BEST MODEL\n",
        "      # COMPUTE METRICS, STORE PARAMETERS\n",
        "    # -------------------- METRICS --------------------\n",
        "      # AFTER RUNNING EVERYTHING, HAVE A FINAL TABLE REPORTING THE PARAMETERS, THE METRICS FOR EACH RUN\n",
        "\n",
        "for i in range(cycles):\n",
        "    print(f\"\\n{'='*30}\\nROUND {i+1}\\n{'='*30}\")\n",
        "\n",
        "    mySVC = svm.SVC(C=1.0, kernel='rbf', gamma = 'scale')\n",
        "\n",
        "    # -------------------- TRAINING --------------------\n",
        "\n",
        "    # -------------- FEATURE EXTRACTION ----------------\n",
        "    training_seqs = pd.Series([], dtype=object)\n",
        "    training_classes = pd.Series([], dtype = object)\n",
        "    for el in training[i]:\n",
        "        df_training = df[df['Label'] == el]\n",
        "        training_seqs = pd.concat([training_seqs, df_training['Frag_90']])\n",
        "        training_classes = pd.concat([training_classes, df_training['Class']])\n",
        "    #training_classes.to_numpy()\n",
        "\n",
        "    #training_seqs = training_seqs.reset_index(drop=True)  # ensure sequential index\n",
        "\n",
        "    training_feats = extract_features(training_seqs)\n",
        "    print(f\"features_{i+1}\")\n",
        "    print(training_feats)\n",
        "    print(\"\")\n",
        "\n",
        "    # -------------------- MODEL FITTING -----------------\n",
        "\n",
        "    mySVC.fit(training_feats, training_classes)\n",
        "\n",
        "    # -------------------- VALIDATION --------------------\n",
        "    sequences_val = df[df['Label'] == validation[i]]['Frag_90']\n",
        "    validation_classes = df[df['Label'] == validation[i]]['Class']\n",
        "    validation_classes.to_numpy()\n",
        "\n",
        "    print(f\"Validation Set Round {i+1}\")\n",
        "    print(sequences_val.head(2))\n",
        "    print(sequences_val.tail(2))\n",
        "\n",
        "    validation_scores = np.array([])\n",
        "    for seq in sequences_val:\n",
        "        score = compute_score(seq, 15, pswm)\n",
        "        validation_scores = np.append(validation_scores, score)\n",
        "\n",
        "    print(f\"Validation sequences processed: {len(validation_scores)}\\n\")\n",
        "\n",
        "    # -------------------- THRESHOLD SELECTION --------------------\n",
        "    precision_vals, recall_vals, thresholds = precision_recall_curve(validation_classes, validation_scores)\n",
        "    fscore = (2 * precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-15)\n",
        "    index = np.argmax(fscore)\n",
        "    optimal_threshold = thresholds[index]\n",
        "\n",
        "    print(f\"Optimal threshold for round {i+1}: {optimal_threshold}\\n\")\n",
        "\n",
        "    # -------------------- TESTING --------------------\n",
        "    testing_sequences = df[df['Label'] == testing[i]]['Frag_90']\n",
        "    testing_classes = df[df['Label'] == testing[i]]['Class']\n",
        "\n",
        "    testing_scores = np.array([compute_score(seq, 15, pswm) for seq in testing_sequences])\n",
        "\n",
        "    predictions = np.where(testing_scores >= optimal_threshold, 1, 0)\n",
        "\n",
        "    print(f\"Testing Set Round {i+1}\")\n",
        "    print(testing_sequences.head(2))\n",
        "    print(testing_sequences.tail(2))\n",
        "    print(f\"Testing samples: {len(testing_sequences)}\\n\")\n",
        "\n",
        "# --- SUMMARY ---\n",
        "print(\"\\n===== CROSS-VALIDATION SUMMARY =====\")\n",
        "print(f\"Accuracies: {accuracies}\")\n",
        "print(f\"Precisions: {precisions}\")\n",
        "print(f\"Recalls: {recalls}\")\n",
        "print(f\"F1-scores: {f1s}\")\n",
        "print(f\"MCCs: {mccs}\")\n",
        "print(f\"Thresholds: {ths}\")\n",
        "print(f\"Confusion matrices: {confusions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbke6apZl2Aj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
