{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVieL8YfuulJ"
      },
      "source": [
        "Import necessary packages and modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3Zraokvuvx0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY5uwrpmvsUH"
      },
      "source": [
        "Define global variables to handle the connection with the UniProt website. We use the Session object to allow retries in case of temporary service unavaiability. We set 5 as tha max number of retries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5PvTl4XFvWK8"
      },
      "outputs": [],
      "source": [
        "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
        "session = requests.Session()\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMKsMgTPv54B"
      },
      "source": [
        "Define functions to handle API calls and pagination (useful when number of entries exceed the current limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "r8griYGrvUR5"
      },
      "outputs": [],
      "source": [
        "def get_next_link(headers):\n",
        "    if \"Link\" in headers:\n",
        "        # The regular expression is used to extract the next link for pagination\n",
        "        re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
        "        match = re_next_link.match(headers[\"Link\"])\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "def get_batch(batch_url):\n",
        "    \"\"\"\"This function retrieves the API link for the next batch\"\"\"\n",
        "    while batch_url:\n",
        "        # Run the API call\n",
        "        response = session.get(batch_url)\n",
        "        # Will raise an error if an error status code is obtained\n",
        "        response.raise_for_status()\n",
        "        # Get the total number of entries in the search\n",
        "        total = response.headers[\"x-total-results\"]\n",
        "        # Yield the response and the total number of entries\n",
        "        yield response, total\n",
        "        # Get the link to the API call for the next data batch\n",
        "        batch_url = get_next_link(response.headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPeCZolUZHy"
      },
      "source": [
        "Let's now test a specific search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "WHOaExvyUjdq"
      },
      "outputs": [],
      "source": [
        "'''FOR REPRODUCIBILITY UPDATE THE CODE IN ORDER TO INSERT THE APIs IN THE COMMAND LINE'''\n",
        "# We defined a basic URL for the search of both the positive and negative datasets.\n",
        "batch_size = 500\n",
        "pos_url = \"https://rest.uniprot.org/uniprotkb/search?format=json&query=%28%28existence%3A1%29+AND+%28length%3A%5B40+TO+*%5D%29+AND+%28reviewed%3Atrue%29+AND+%28fragment%3Afalse%29+AND+%28taxonomy_id%3A2759%29+AND+%28ft_signal_exp%3A*%29%29&size=500\"\n",
        "neg_url = \"https://rest.uniprot.org/uniprotkb/search?format=json&query=%28%28existence%3A1%29+AND+%28length%3A%5B40+TO+*%5D%29+AND+%28reviewed%3Atrue%29+AND+%28fragment%3Afalse%29+AND+%28taxonomy_id%3A2759%29+NOT+%28ft_signal%3A*%29+AND+%28%28cc_scl_term_exp%3ASL-0091%29+OR+%28cc_scl_term_exp%3ASL-0191%29+OR+%28cc_scl_term_exp%3ASL-0173%29+OR+%28cc_scl_term_exp%3ASL-0204%29+OR+%28cc_scl_term_exp%3ASL-0209%29+OR+%28cc_scl_term_exp%3ASL-0039%29%29%29&size=500\"\n",
        "\n",
        "def filter_pos_entry(entry):\n",
        "    # We iterate over the features of the entry\n",
        "    for feature in entry[\"features\"]:\n",
        "        # We only consider features of type Signal\n",
        "        if feature[\"type\"] == \"Signal\" and feature[\"description\"]==\"\":\n",
        "            # Check if the coiled-coil starts before position 100\n",
        "            if feature[\"location\"][\"end\"][\"value\"] >= 14:\n",
        "              return True\n",
        "    return False\n",
        "\n",
        "'''FOR REPRODUCIBILITY UPDATE THE CODE IN ORDER TO INSERT THE FILENAMES IN THE COMMAND LINE'''\n",
        "# We set the name of the output files, we want TSV output\n",
        "output_pos_file = \"eukarya_SP_pos.tsv\"\n",
        "output_neg_file = \"eukarya_SP_neg.tsv\"\n",
        "pos_fasta = \"pos.fasta\"\n",
        "neg_fasta = \"neg.fasta\"\n",
        "\n",
        "# We define a function to better control the TSV format in output.\n",
        "# In particular, we run the API call requiring JSON format and build our own TSV file\n",
        "# To this aim, the following function extract and process specific fields from the JSON file\n",
        "\n",
        "def extract_pos_fields(entry):\n",
        "    '''This function better controls the TSV format in output. In particular, we run the API call requiring JSON format\n",
        "    and build our own TSV file. To this aim, the following function extract and process specific fields from the JSON file'''\n",
        "    # We extract the accession, the sequence length and the start and end location of the first coiled-coil segment\n",
        "    e = 0\n",
        "    # We iterate over the features of the entry\n",
        "    for f in entry[\"features\"]:\n",
        "        # We only consider the first coiled-coil segment\n",
        "        if f[\"type\"] == \"Signal\" and f[\"description\"]==\"\":\n",
        "            if f[\"location\"][\"end\"][\"value\"] >= 14:\n",
        "              e = f[\"location\"][\"end\"][\"value\"]\n",
        "    return (entry[\"primaryAccession\"], entry[\"organism\"][\"scientificName\"], entry[\"organism\"][\"lineage\"][1], entry[\"sequence\"][\"length\"], e)\n",
        "\n",
        "def extract_neg_fields(entry):\n",
        "    '''This function better controls the TSV format in output. In particular, we run the API call requiring JSON format\n",
        "    and build our own TSV file. To this aim, the following function extract and process specific fields from the JSON file'''\n",
        "    hel_trs = False\n",
        "    for f in entry[\"features\"]:\n",
        "        # We only consider the first coiled-coil segment\n",
        "        if f[\"type\"] == \"Transmembrane\" and f[\"description\"]==\"Helical\" and f[\"location\"][\"start\"][\"value\"] <= 90:\n",
        "          hel_trs = True\n",
        "          break\n",
        "    return (entry[\"primaryAccession\"], entry[\"organism\"][\"scientificName\"], entry[\"organism\"][\"lineage\"][1], entry[\"sequence\"][\"length\"], hel_trs)\n",
        "\n",
        "def extract_seq(entry):\n",
        "  return (\">\" + str(entry[\"primaryAccession\"]) + \"\\n\" + str(entry[\"sequence\"][\"value\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf1tqlG9wmth"
      },
      "source": [
        "Define a function to retrieve the datset and format it in TSV. It takes in input:\n",
        "\n",
        "*   a search URL corresponding the the specific API call to run (to be build using the UniProt website)\n",
        "*   a filter function to filter entries according to specific criteria\n",
        "*   and extract function used to get specific fields of interest from the JSON format\n",
        "*   the output file name where the final TSV will be saved\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "SA3VmKetYKQh"
      },
      "outputs": [],
      "source": [
        "# We have to insert in place of the parameters:\n",
        "# 2 search_url (pos_url, neg_url);\n",
        "# filter_function = filter_pos_entry();\n",
        "# extract_function = extract_pos_fields() and extract_neg_fields();\n",
        "# output_file_name = output_pos_file; output_neg_file\n",
        "\n",
        "\n",
        "# positive dataset:\n",
        "\n",
        "def get_pos_dataset(search_url, filter_function, extract_function, output_file_name, out_pos_fasta):\n",
        "    filtered_json = []\n",
        "    filtered_fasta = \"\"\n",
        "    n_total, n_filtered = 0, 0\n",
        "    # Run the API call in batches\n",
        "    for batch, total in get_batch(search_url):\n",
        "        # parse the JSON body of the response\n",
        "        #print(\"Ok1\")\n",
        "        #print(batch.text)\n",
        "        batch_json = json.loads(batch.text)\n",
        "        #print(\"Ok\")\n",
        "        # filter the entries\n",
        "        for entry in batch_json[\"results\"]:\n",
        "            n_total += 1\n",
        "            # Check if the entry passes the filter\n",
        "            if filter_function(entry):\n",
        "                n_filtered += 1\n",
        "                filtered_json.append(extract_function(entry))\n",
        "                filtered_fasta += extract_seq(entry) + \"\\n\"\n",
        "    with open(out_pos_fasta, \"w\") as ofs:\n",
        "        ofs.write(filtered_fasta)\n",
        "        ofs.close\n",
        "    print(n_total, n_filtered)\n",
        "    with open(output_file_name, \"w\") as ofs:\n",
        "        for entry in filtered_json:\n",
        "            # Extract the fields of interest\n",
        "            '''fields = extract_function(entry)'''\n",
        "            # Print the fields in TSV format\n",
        "            print(*entry, sep=\"\\t\", file=ofs)\n",
        "        ofs.close\n",
        "\n",
        "# negative dataset:\n",
        "\n",
        "def get_neg_dataset(search_url, extract_function, output_file_name, out_neg_fasta): # the filter function parameter has been removed\n",
        "    neg_json = []\n",
        "    filtered_fasta = \"\"\n",
        "    n_total = 0 #, n_filtered = 0, 0\n",
        "    # Run the API call in batches\n",
        "    for batch, total in get_batch(search_url):\n",
        "        # parse the JSON body of the response\n",
        "        batch_json = json.loads(batch.text)\n",
        "        # filter the entries is not necessary here - I suppose\n",
        "        for entry in batch_json[\"results\"]:\n",
        "            n_total += 1\n",
        "            neg_json.append(extract_function(entry))\n",
        "            filtered_fasta += extract_seq(entry) + \"\\n\"\n",
        "    with open(out_neg_fasta, \"w\") as ofs:\n",
        "        ofs.write(filtered_fasta)\n",
        "        ofs.close\n",
        "    print(n_total)\n",
        "    with open(output_file_name, \"w\") as ofs:\n",
        "        for entry in neg_json:\n",
        "            # Extract the fields of interest\n",
        "            '''fields = extract_function(entry)'''\n",
        "            # Print the fields in TSV format\n",
        "            print(*entry, sep=\"\\t\", file=ofs)\n",
        "        ofs.close\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2cDk_YYwYR3"
      },
      "source": [
        "We call the above function to obtaine our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlTkNelqYWGE",
        "outputId": "77e755e0-160e-45a0-f263-e15a51ab248a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2949 2932\n",
            "20615\n"
          ]
        }
      ],
      "source": [
        "# Here we call the 2 new functions:\n",
        "\n",
        "get_pos_dataset(pos_url, filter_pos_entry, extract_pos_fields, output_pos_file, pos_fasta) #need to edit output_file variable and create 2\n",
        "get_neg_dataset(neg_url, extract_neg_fields, output_neg_file, neg_fasta) # as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jTAF4loLf4Wd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}